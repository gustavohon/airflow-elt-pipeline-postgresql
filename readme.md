Pipeline de Dados para An√°lise Financeira com Airflow e Docker
üéØ Objetivo do Projeto
Este projeto demonstra a constru√ß√£o de um pipeline de dados ELT (Extract, Load, Transform) de ponta a ponta, simulando um cen√°rio real de engenharia de dados para uma institui√ß√£o financeira.

O objetivo principal √© consolidar dados de fontes heterog√™neas ‚Äî um banco de dados transacional (PostgreSQL) e arquivos de dados brutos (CSV) ‚Äî em um Data Warehouse centralizado, pronto para an√°lises e business intelligence. Todo o processo √© orquestrado e automatizado com o Apache Airflow, garantindo que o pipeline seja robusto, agend√°vel e idempotente.

üèóÔ∏è Arquitetura e Fluxo de Dados
O pipeline foi projetado para ser eficiente e escal√°vel, seguindo as melhores pr√°ticas de engenharia de dados:

Extra√ß√£o Paralela:

Os dados s√£o extra√≠dos simultaneamente de duas fontes para otimizar o tempo de execu√ß√£o:

Banco de Dados Relacional: Informa√ß√µes cadastrais de clientes, ag√™ncias, contas e colaboradores de um banco de dados PostgreSQL.

Arquivos CSV: Dados hist√≥ricos de transa√ß√µes financeiras.

Staging (√Årea de Passagem):

Os dados brutos extra√≠dos s√£o padronizados para o formato CSV e salvos em uma √°rea de passagem (staging area). Os arquivos s√£o organizados em diret√≥rios com a data da execu√ß√£o, criando um hist√≥rico e facilitando a depura√ß√£o.

Carregamento (Load):

Ap√≥s a conclus√£o bem-sucedida de ambas as extra√ß√µes, uma tarefa de carregamento √© acionada.

Ela l√™ os arquivos da √°rea de passagem e os insere em tabelas correspondentes em um Data Warehouse PostgreSQL dedicado.

A l√≥gica de carregamento utiliza if_exists='replace' para garantir a idempot√™ncia, permitindo que o pipeline seja re-executado para o mesmo dia sem risco de duplicar dados.

üõ†Ô∏è Tecnologias Utilizadas
Orquestra√ß√£o: Apache Airflow

Cont√™ineres: Docker e Docker Compose

Linguagem: Python

Bancos de Dados: PostgreSQL (tanto como fonte quanto como Data Warehouse)

Bibliotecas Python: pandas para manipula√ß√£o de dados, psycopg2 e SQLAlchemy para a intera√ß√£o com os bancos de dados.

‚ñ∂Ô∏è Como Executar o Projeto
Pr√©-requisitos:

Docker

Docker Compose

Passo a passo para a execu√ß√£o:

Clone o reposit√≥rio para a sua m√°quina.

Inicie o Ambiente Docker:
Abra um terminal na pasta raiz do projeto e execute o comando abaixo. Ele ir√° construir e iniciar todos os servi√ßos necess√°rios (o banco de dados fonte, o Airflow e o Data Warehouse).

Bash

docker-compose up -d --build
Aguarde alguns minutos para que todos os servi√ßos do Airflow estejam completamente no ar.

Acesse a Interface do Airflow:
Abra seu navegador e acesse http://localhost:8080. As credenciais padr√£o s√£o:

Login: airflow

Senha: airflow

Execute o Pipeline:

Na interface do Airflow, localize a DAG pipeline_dados_indicium.

Ative-a utilizando o bot√£o de "toggle" √† esquerda.

Inicie uma execu√ß√£o manual clicando no bot√£o de "Play" (‚ñ∂Ô∏è) √† direita.

‚úÖ Verificando o Resultado
Ap√≥s a execu√ß√£o bem-sucedida da DAG (todas as tarefas ficar√£o verdes na interface), voc√™ pode confirmar o resultado:

Arquivos de Staging:
Uma pasta output/ ser√° criada na raiz do projeto, contendo os arquivos CSV extra√≠dos e organizados por data.

Dados no Data Warehouse:
Utilize um cliente de banco de dados de sua prefer√™ncia para se conectar ao Data Warehouse:

Host: localhost

Porta: 5433

Usu√°rio: dw_user

Senha: dw_password

Banco de Dados: data_warehouse

Dentro do banco, voc√™ encontrar√° as cinco tabelas (agencias, clientes, colaboradores, contas e transacoes) populadas com os dados processados pelo pipeline.
